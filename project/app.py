from flask import Flask, request, jsonify, render_template, send_from_directory
from io import BytesIO
from PIL import Image
import base64
import os
import time
import torch

from transformers import BlipProcessor, BlipForConditionalGeneration
from peft import PeftModel
from gtts import gTTS
from openai import OpenAI

# -----------------------------
#  ê²½ë¡œ / í™˜ê²½ ì„¤ì •
# -----------------------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

BASE_MODEL = "Salesforce/blip-image-captioning-base"

# LoRA ê²½ë¡œ (app.py ê¸°ì¤€)
LORA_DIR = os.path.join(BASE_DIR, "blip_lora_ko")
ADAPTER_DIR = os.path.join(LORA_DIR, "adapter")
PROCESSOR_DIR = os.path.join(LORA_DIR, "processor")

# ğŸ”Š TTS íŒŒì¼ì€ static/tts ë°‘ì— ì €ì¥
TTS_DIR = os.path.join(BASE_DIR, "static", "tts")
os.makedirs(TTS_DIR, exist_ok=True)

DEVICE = torch.device("cpu")

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

# Flask ì•±
app = Flask(__name__, static_folder="static", static_url_path="/")

# OpenAI LLM í´ë¼ì´ì–¸íŠ¸
llm_client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None


# -----------------------------
#  BLIP + LoRA ë¡œë”©
# -----------------------------
def load_model():
    """
    1) BLIP base ëª¨ë¸ ë¡œë“œ
    2) LoRA adapterë¥¼ merge
    3) processor ë¡œë“œ
    """
    print("ğŸ”„ BLIP + LoRA ëª¨ë¸ ë¡œë”© ì¤‘...")

    # processor
    if os.path.isdir(PROCESSOR_DIR):
        processor = BlipProcessor.from_pretrained(PROCESSOR_DIR)
    else:
        processor = BlipProcessor.from_pretrained(BASE_MODEL)

    # base ëª¨ë¸
    base_model = BlipForConditionalGeneration.from_pretrained(BASE_MODEL)

    # LoRA ì–´ëŒ‘í„° merge
    if os.path.isdir(ADAPTER_DIR):
        print("ğŸ”¹ LoRA ì–´ëŒ‘í„° ì ìš©...")
        lora_model = PeftModel.from_pretrained(base_model, ADAPTER_DIR)
        model = lora_model.merge_and_unload()
    else:
        print("âš  adapter í´ë” ì—†ìŒ â†’ base ëª¨ë¸ë§Œ ì‚¬ìš©")
        model = base_model

    model.to(DEVICE)
    model.eval()
    print("âœ… BLIP ë¡œë”© ì™„ë£Œ!")
    return processor, model


processor, blip_model = load_model()


# -----------------------------
#  ìœ í‹¸ í•¨ìˆ˜ë“¤
# -----------------------------
def blip_caption_from_base64(image_b64: str) -> str:
    """Base64 ì´ë¯¸ì§€ì—ì„œ BLIP ìº¡ì…˜ ë½‘ê¸°"""
    img_bytes = base64.b64decode(image_b64)
    pil_image = Image.open(BytesIO(img_bytes)).convert("RGB")

    inputs = processor(images=pil_image, return_tensors="pt").to(DEVICE)
    with torch.no_grad():
        output_ids = blip_model.generate(
            **inputs,
            max_length=40,
            num_beams=5,
            no_repeat_ngram_size=2,
        )

    caption = processor.decode(output_ids[0], skip_special_tokens=True).strip()
    print("[BLIP ìº¡ì…˜]", caption)
    return caption


def make_korean_caption(raw_caption: str) -> str:
    """
    BLIPê°€ ë½‘ì€ ìº¡ì…˜(raw_caption)ì„
    ì‹œê°ì¥ì• ì¸ì´ ë“£ê¸° ì¢‹ì€ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ í•œë‘ ë¬¸ì¥ìœ¼ë¡œ ì •ë¦¬.
    (OPENAI_API_KEY ì—†ìœ¼ë©´ ê·¸ëƒ¥ ì›ë¬¸ ì‚¬ìš©)
    """
    if llm_client is None:
        return raw_caption

    completion = llm_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {
                "role": "system",
                "content": (
                    "ë„ˆëŠ” ì‹œê°ì¥ì• ì¸ì„ ìœ„í•œ í™”ë©´ ì„¤ëª… ë„ìš°ë¯¸ì•¼. "
                    "ì…ë ¥ëœ ë¬¸ì¥ì„ ë°”íƒ•ìœ¼ë¡œ, ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ í•œë‘ ë¬¸ì¥ìœ¼ë¡œ "
                    "ì¡´ëŒ“ë§ë¡œ ì„¤ëª…í•´ì¤˜. êµ°ë”ë”ê¸° ì—†ì´ í•µì‹¬ë§Œ ë§í•´."
                ),
            },
            {
                "role": "user",
                "content": f"ë‹¤ìŒ ìº¡ì…˜ì„ í•œêµ­ì–´ë¡œ ì •ë¦¬í•´ì¤˜: {raw_caption}",
            },
        ],
    )
    text = completion.choices[0].message.content.strip()
    print("[í•œêµ­ì–´ ìº¡ì…˜]", text)
    return text


def save_tts_korean(text: str, filename: str) -> str:
    """ê°„ë‹¨ TTS ìƒì„± (gTTS í•œêµ­ì–´) -> static/tts/filename.mp3"""
    path = os.path.join(TTS_DIR, filename)
    print(f"[TTS] ì €ì¥ ê²½ë¡œ: {path}")
    tts = gTTS(text=text, lang="ko")
    tts.save(path)
    return path


def stt_korean_file(audio_file) -> str:
    """
    ì—…ë¡œë“œëœ ì˜¤ë””ì˜¤ íŒŒì¼(ì›¹m ë“±)ì„ Whisperë¡œ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜.
    """
    if llm_client is None:
        return ""

    tmp_name = f"voice_{int(time.time())}.webm"
    tmp_path = os.path.join(TTS_DIR, tmp_name)
    audio_file.save(tmp_path)
    print(f"[STT] ì„ì‹œ ì˜¤ë””ì˜¤ ì €ì¥: {tmp_path}")

    with open(tmp_path, "rb") as f:
        result = llm_client.audio.transcriptions.create(
            model="whisper-1",
            file=f,
            language="ko",
        )
    print("[STT ê²°ê³¼]", result.text)
    return result.text.strip()


# -----------------------------
#  ë¼ìš°í„°
# -----------------------------
@app.route("/")
def index():
    # templates/index.html ë Œë”ë§
    return render_template("index.html")


# ğŸ”Š TTS mp3 ì„œë¹™
@app.route("/tts/<filename>")
def serve_tts(filename):
    print(f"[TTS ì„œë¹™ ìš”ì²­] {filename}")
    return send_from_directory(TTS_DIR, filename)


# -----------------------------
# 1) ìº¡ì…˜: ì§€ê¸ˆ ì¥ë©´ ì„¤ëª… + í•œêµ­ì–´ TTS
# -----------------------------
@app.route("/api/caption", methods=["POST"])
def api_caption():
    data = request.get_json()
    image_b64 = data.get("image")

    if not image_b64:
        return jsonify({"error": "image field not found"}), 400

    # dataURL í˜•ì‹ì¼ ê²½ìš° ì•ë¶€ë¶„ ì œê±°
    if "," in image_b64:
        image_b64 = image_b64.split(",", 1)[1]

    try:
        raw_caption = blip_caption_from_base64(image_b64)
    except Exception as e:
        print("[ERROR] caption error:", e)
        return jsonify({"error": f"caption error: {e}"}), 500

    korean_caption = make_korean_caption(raw_caption)

    # í•œêµ­ì–´ ì„¤ëª…ì„ TTSë¡œ ì½ì–´ì£¼ê¸°
    tts_url = None
    try:
        filename = "caption.mp3"
        tts_path = save_tts_korean(korean_caption, filename)
        tts_url = f"/tts/{filename}"
        print("[TTS URL]", tts_url)
    except Exception as e:
        print("[ERROR] TTS ìƒì„± ì‹¤íŒ¨:", e)
        tts_url = None

    return jsonify(
        {
            "raw_caption": raw_caption,
            "korean_caption": korean_caption,
            "tts_url": tts_url,
        }
    )


# -----------------------------
# 2) í…ìŠ¤íŠ¸ ì±„íŒ… Q&A
# -----------------------------
@app.route("/api/ask", methods=["POST"])
def api_ask():
    if llm_client is None:
        return jsonify(
            {
                "answer": "LLM API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. "
                "í„°ë¯¸ë„ì—ì„œ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ ì£¼ì„¸ìš”.",
                "error": True,
            }
        )

    data = request.get_json()
    question = (data.get("question") or "").strip()
    image_b64 = data.get("image")

    if not question:
        return jsonify({"answer": "ì§ˆë¬¸ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.", "error": True})

    if not image_b64:
        return jsonify({"answer": "ì´ë¯¸ì§€ê°€ ì „ì†¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "error": True})

    if "," in image_b64:
        image_b64 = image_b64.split(",", 1)[1]

    try:
        response = llm_client.responses.create(
            model="gpt-4o-mini",
            input=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "input_text",
                            "text": (
                                "ë„ˆëŠ” ì‹œê°ì¥ì• ì¸ì„ ìœ„í•œ ì¥ë©´ ì„¤ëª… ë„ìš°ë¯¸ì•¼. "
                                "ì•„ë˜ ì´ë¯¸ì§€ë¥¼ ë³´ê³ , ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ "
                                "í•œêµ­ì–´ë¡œ 1~2ë¬¸ì¥ ì •ë„ë¡œ ì§§ê³  ë¶„ëª…í•˜ê²Œ ëŒ€ë‹µí•´ ì¤˜.\n\n"
                                f"ì§ˆë¬¸: {question}"
                            ),
                        },
                        {
                            "type": "input_image",
                            "image_url": "data:image/jpeg;base64," + image_b64,
                        },
                    ],
                }
            ],
        )

        answer_text = response.output[0].content[0].text.strip()
        print("[í…ìŠ¤íŠ¸ Q&A ë‹µë³€]", answer_text)

    except Exception as e:
        print("[ERROR] LLM í˜¸ì¶œ ì‹¤íŒ¨:", e)
        return jsonify(
            {"answer": f"LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}", "error": True}
        )

    # ë‹µë³€ë„ TTSë¡œ ì½ì–´ì£¼ê¸°
    tts_url = None
    try:
        filename = f"answer_{int(time.time())}.mp3"
        save_tts_korean(answer_text, filename)
        tts_url = f"/tts/{filename}"
        print("[Q&A TTS URL]", tts_url)
    except Exception as e:
        print("[ERROR] Q&A TTS ì‹¤íŒ¨:", e)
        tts_url = None

    return jsonify({"answer": answer_text, "error": False, "tts_url": tts_url})


# -----------------------------
# 3) ìŒì„± Q&A: ìŒì„± â†’ STT â†’ Vision Q&A
# -----------------------------
@app.route("/api/voice-ask", methods=["POST"])
def api_voice_ask():
    if llm_client is None:
        return jsonify(
            {
                "answer": "OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                "error": True,
            }
        )

    audio_file = request.files.get("audio")
    image_b64 = request.form.get("image")

    if not audio_file:
        return jsonify({"answer": "ì˜¤ë””ì˜¤ê°€ ì „ì†¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "error": True})

    if not image_b64:
        return jsonify({"answer": "ì´ë¯¸ì§€ê°€ ì „ì†¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "error": True})

    # 1) STTë¡œ ì§ˆë¬¸ í…ìŠ¤íŠ¸ ì–»ê¸°
    try:
        question_text = stt_korean_file(audio_file)
        if not question_text:
            return jsonify({"answer": "ìŒì„±ì„ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", "error": True})
    except Exception as e:
        print("[ERROR] STT ì˜¤ë¥˜:", e)
        return jsonify({"answer": f"STT ì˜¤ë¥˜: {e}", "error": True})

    # 2) base64 í—¤ë” ì œê±°
    if "," in image_b64:
        image_b64 = image_b64.split(",", 1)[1]

    # 3) Vision Q&A í˜¸ì¶œ
    try:
        response = llm_client.responses.create(
            model="gpt-4o-mini",
            input=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "input_text",
                            "text": (
                                "ë„ˆëŠ” ì‹œê°ì¥ì• ì¸ì„ ìœ„í•œ ì¥ë©´ ì„¤ëª… ë„ìš°ë¯¸ì•¼. "
                                "ì•„ë˜ ì´ë¯¸ì§€ë¥¼ ë³´ê³ , ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ "
                                "í•œêµ­ì–´ë¡œ 1~2ë¬¸ì¥ ì •ë„ë¡œ ì§§ê³  ë¶„ëª…í•˜ê²Œ ëŒ€ë‹µí•´ ì¤˜.\n\n"
                                f"ì§ˆë¬¸: {question_text}"
                            ),
                        },
                        {
                            "type": "input_image",
                            "image_url": "data:image/jpeg;base64," + image_b64,
                        },
                    ],
                }
            ],
        )

        answer_text = response.output[0].content[0].text.strip()
        print("[ìŒì„± Q&A ë‹µë³€]", answer_text)

    except Exception as e:
        print("[ERROR] LLM(voice) í˜¸ì¶œ ì‹¤íŒ¨:", e)
        return jsonify({"answer": f"LLM ì˜¤ë¥˜: {e}", "error": True})

    # 4) ë‹µë³€ë„ TTSë¡œ ì½ì–´ì£¼ê¸°
    tts_url = None
    try:
        filename = f"voice_answer_{int(time.time())}.mp3"
        save_tts_korean(answer_text, filename)
        tts_url = f"/tts/{filename}"
        print("[VOICE Q&A TTS URL]", tts_url)
    except Exception as e:
        print("[ERROR] VOICE TTS ì‹¤íŒ¨:", e)
        tts_url = None

    return jsonify(
        {
            "question": question_text,
            "answer": answer_text,
            "tts_url": tts_url,
            "error": False,
        }
    )


# -----------------------------
# ë©”ì¸
# -----------------------------
if __name__ == "__main__":
    # ê°œë°œìš© ì„œë²„ ì‹¤í–‰
    app.run(host="0.0.0.0", port=5000, debug=True)
